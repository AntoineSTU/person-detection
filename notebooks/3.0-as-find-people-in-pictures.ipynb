{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find people on pictures\n",
    "\n",
    "Now that we have trained models, we will try to find the localization of people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utility_functions import log_progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CSV_PATH = \"../data/raw/RAISE_6k.csv\"\n",
    "MODEL_PATH = \"../models/trained_model_pooling_2000\"\n",
    "IMAGE_SIZE = (600, 700)\n",
    "NB_IMAGES_TO_DL = 50\n",
    "IS_LAYER_MODEL = False\n",
    "PEOPLE_PRED_VALUE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pictures download and preprocessing\n",
    "\n",
    "Here we will download some pictures and extract the ones with people in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "DATASET_SIZE = NB_IMAGES_TO_DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we extract from the dataframe:\n",
    "- The file name\n",
    "- The download link\n",
    "- The people label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"Class\"] = df[\"Keywords\"].map(lambda keywordsStr : 'people' if 'people' in keywordsStr else 'not_people')\n",
    "df[\"FileName\"] = df[\"File\"].map(lambda file_name : \"image_\" + file_name + \".tif\")\n",
    "df = df[[\"FileName\", \"TIFF\", \"Class\"]]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we select the correct number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df[df[\"Class\"] == \"people\"][:DATASET_SIZE]\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can download the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_path(file_name):\n",
    "    return \"../data/raw/pictures/\" + file_name\n",
    "\n",
    "def download_images():\n",
    "    for index, row in log_progress(df_final.iterrows(), size=df_final.shape[0]):\n",
    "        if not os.path.isfile(get_file_path(row[\"FileName\"])):\n",
    "            response = requests.get(row[\"TIFF\"])\n",
    "            file = open(get_file_path(row[\"FileName\"]), 'wb')\n",
    "            file.write(response.content)\n",
    "            file.close()\n",
    "\n",
    "download_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The goal of this part is to preprocess data that we will use in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must do the same preprocessing that we did for our model (essentially resize and put the color values between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for index, row in log_progress(df_final.iterrows(), size=DATASET_SIZE):\n",
    "    img = image.load_img(get_file_path(row[\"FileName\"]), target_size=IMAGE_SIZE)\n",
    "    images.append(image.img_to_array(img))\n",
    "preprocessed_images = preprocess_input(np.array(images))\n",
    "print(preprocessed_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and test model\n",
    "\n",
    "Now we will load the model and keep only images that are detected with people on them.\n",
    "\n",
    "**Remark:** depending on how the model was built, the inception layers can be accessible or hidden in a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(MODEL_PATH)\n",
    "print(\"Model structure: \", model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nb of layers\n",
    "print(\"Nb of layers: \", len(model.layers), \"\\n\")\n",
    "\n",
    "if not IS_LAYER_MODEL:\n",
    "    # Entry layer\n",
    "    print(\"Entry layer: \", model.layers[0], \" with \", model.layers[0].input_shape, \" input shape.\\n\")\n",
    "\n",
    "    # Last layers\n",
    "    print(\"Before before last layer: \", model.layers[-3], \" with \", model.layers[-3].output_shape, \" output shape.\")\n",
    "    print(\"Before last layer: \", model.layers[-2], \" with \", model.layers[-2].output_shape, \" output shape.\")\n",
    "    print(\"Last layer: \", model.layers[-1], \" with \", model.layers[-1].output_shape, \" output shape.\")\n",
    "\n",
    "else:\n",
    "    # Model layers\n",
    "    print(\"First layer: \", model.layers[0], \" with \", model.layers[0].input_shape, \" input shape.\\n\")\n",
    "    print(\"Second layer (Inception V3): \", model.layers[1], \" with \", model.layers[0].output_shape, \" input shape.\\n\")\n",
    "    print(\"Third layer: \", model.layers[2], \" with \", model.layers[2].input_shape, \" input shape.\\n\\n\")\n",
    "\n",
    "    # Inside Inception v3\n",
    "    print(\"First Inception layer: \", model.layers[1].layers[0], \" with \", model.layers[1].layers[0].input_shape, \" input shape.\\n\")\n",
    "    print(\"Before last Inception layer: \", model.layers[1].layers[-2], \" with \", model.layers[1].layers[-2].output, \" output.\\n\")\n",
    "    print(\"Last Inception layer: \", model.layers[1].layers[-1], \" with \", model.layers[1].layers[-1].output, \" output.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we predict labels from the preprocessed images (all with people)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(preprocessed_images, batch_size=10, verbose=1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.where(predictions > 0.5, PEOPLE_PRED_VALUE, 1-PEOPLE_PRED_VALUE)\n",
    "print(\"Accuracy: {}\".format(sum(predictions)[0]/len(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model to find the position of the people\n",
    "\n",
    "Now we will try to find the position of the people by extracting the final dense layer weights, deleting the average pooling and the final dense layer, and adding a 1*1 convolution layer with these weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we extract the weights from the final layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer = model.layers[-1]\n",
    "dense_layer_weights = dense_layer.get_weights()\n",
    "dense_layer_weights[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we construct the new model:\n",
    "- We remove the last two layers (pooling and dense)\n",
    "- We add a 2D 1*1 convolution layer with the extracted weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "if not IS_LAYER_MODEL:\n",
    "    formatted_weights = (np.array([[dense_layer_weights[0]]]), dense_layer_weights[1])\n",
    "    prediction_outputs = Conv2D(1, (1,1), activation='sigmoid', weights=formatted_weights, name='conv_predictor')(model.layers[-3].output)\n",
    "else:\n",
    "    print(\"Not implemented!\")\n",
    "prediction_model = Model(inputs=model.inputs, outputs=prediction_outputs)\n",
    "prediction_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "print(prediction_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nb of layers\n",
    "print(\"Nb of layers: \", len(prediction_model.layers), \"\\n\")\n",
    "\n",
    "# Entry layer\n",
    "print(\"Entry layer: \", prediction_model.layers[0], \" with \", prediction_model.layers[0].input_shape, \" input shape.\\n\")\n",
    "\n",
    "# Last layers\n",
    "print(\"Before last layer: \", prediction_model.layers[-2], \" with \", prediction_model.layers[-2].output_shape, \" output shape.\")\n",
    "print(\"Last layer: \", prediction_model.layers[-1], \" with \", prediction_model.layers[-1].output_shape, \" output shape.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing step\n",
    "\n",
    "Now let's try this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform as st\n",
    "from math import ceil\n",
    "\n",
    "IMG_IND = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_picture = np.array([preprocessed_images[IMG_IND]])\n",
    "prediction_new_model = prediction_model.predict(first_picture)\n",
    "prediction_new_model = prediction_new_model[0].reshape(prediction_new_model[0].shape[0], prediction_new_model[0].shape[1])\n",
    "print(prediction_new_model.shape)\n",
    "#prediction_new_model = np.where(prediction_new_model > 0.2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction_new_model)\n",
    "img = image.load_img(get_file_path(df_final.iloc[IMG_IND][\"FileName\"]), target_size=IMAGE_SIZE)\n",
    "plt.imshow(np.asarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_colored(img, predictions):\n",
    "    def to_red_green(x):\n",
    "        if PEOPLE_PRED_VALUE == 0:\n",
    "            color_x = np.array([0, 255, 0])\n",
    "            color_1_x = np.array([255, 0, 0])\n",
    "        else:\n",
    "            color_x = np.array([255, 0, 0])\n",
    "            color_1_x = np.array([0, 255, 0])\n",
    "        return x * color_x + (1-x) * color_1_x\n",
    "    red_green_img = np.array([[to_red_green(x) for x in row] for row in predictions])\n",
    "    red_green_img_resized = 255*st.resize(red_green_img.astype('uint8'), IMAGE_SIZE)\n",
    "\n",
    "    colored_image = 0.5*np.asarray(img) + 0.5*red_green_img_resized\n",
    "    return colored_image.astype('uint8')\n",
    "\n",
    "plt.imshow(get_img_colored(img, prediction_new_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we can see the results for all the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_new_model = prediction_model.predict(preprocessed_images, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_IMAGES_PER_ROW = 4\n",
    "fig, axes = plt.subplots(nrows=ceil(NB_IMAGES_TO_DL/NB_IMAGES_PER_ROW), ncols=NB_IMAGES_PER_ROW, figsize=(NB_IMAGES_PER_ROW*10,ceil(NB_IMAGES_TO_DL/NB_IMAGES_PER_ROW)*10))\n",
    "plt.figure(1)\n",
    "for i, pred in enumerate(predictions_new_model):\n",
    "    #formatted_prediction = np.where(pred > 0.5, PEOPLE_PRED_VALUE, 1-PEOPLE_PRED_VALUE)\n",
    "    formatted_prediction = pred\n",
    "    formatted_prediction = formatted_prediction.reshape(formatted_prediction.shape[0], formatted_prediction.shape[1])\n",
    "    img = image.load_img(get_file_path(df_final.iloc[i][\"FileName\"]), target_size=IMAGE_SIZE)\n",
    "    colored_image = get_img_colored(img, formatted_prediction)\n",
    "    axes[i//NB_IMAGES_PER_ROW, i%NB_IMAGES_PER_ROW].imshow(colored_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here what parts of the picture are detected as people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Now we will try to improve the visualizations of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform as st\n",
    "from math import ceil\n",
    "\n",
    "IMG_IND = 10\n",
    "THRESHOLD = 0.5\n",
    "PROP_COLOR = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = predictions_new_model[IMG_IND]\n",
    "test_pred = test_pred.reshape(test_pred.shape[0], test_pred.shape[1])\n",
    "test_img = image.load_img(get_file_path(df_final.iloc[IMG_IND][\"FileName\"]), target_size=IMAGE_SIZE)\n",
    "print(test_pred.shape)\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_pred(predictions):\n",
    "    if PEOPLE_PRED_VALUE == 1:\n",
    "        predictions = 1-predictions\n",
    "    return np.where(predictions > THRESHOLD, 1, 0)\n",
    "\n",
    "thresholded_test_predictions = threshold_pred(test_pred)\n",
    "thresholded_test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_visualization(img, predictions):\n",
    "    img_array = np.asarray(img).copy()\n",
    "    width_img, height_img = img.size\n",
    "    height_pred, width_pred = predictions.shape\n",
    "    i_step = height_img/height_pred\n",
    "    j_step = width_img/width_pred\n",
    "    for i in range(predictions.shape[0]):\n",
    "        for j in range(predictions.shape[1]):\n",
    "            if predictions[i, j] == 1:\n",
    "                i_0 = int(i*i_step)\n",
    "                i_1 = int((i+1)*i_step)\n",
    "                j_0 = int(j*j_step)\n",
    "                j_1 = int((j+1)*j_step)\n",
    "                green_array = np.array([[[0, 255, 0] for k in range(j_1-j_0)] for l in range(i_1-i_0)])\n",
    "                img_array[i_0:i_1, j_0:j_1] = (1-PROP_COLOR) * img_array[i_0:i_1, j_0:j_1] + PROP_COLOR * green_array\n",
    "    return img_array.astype('uint8')\n",
    "\n",
    "img_pred = get_img_visualization(test_img, thresholded_test_predictions)\n",
    "plt.imshow(img_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thresholded_visualization(img, predictions):\n",
    "    reshaped_pred = predictions.reshape(predictions.shape[0], predictions.shape[1])\n",
    "    thresholded_pred = threshold_pred(reshaped_pred)\n",
    "    return get_img_visualization(img, thresholded_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_IMAGES_PER_ROW = 4\n",
    "fig, axes = plt.subplots(nrows=ceil(NB_IMAGES_TO_DL/NB_IMAGES_PER_ROW), ncols=NB_IMAGES_PER_ROW, figsize=(NB_IMAGES_PER_ROW*10,ceil(NB_IMAGES_TO_DL/NB_IMAGES_PER_ROW)*10))\n",
    "plt.figure(1)\n",
    "for i, pred in enumerate(predictions_new_model):\n",
    "    img = image.load_img(get_file_path(df_final.iloc[i][\"FileName\"]), target_size=IMAGE_SIZE)\n",
    "    visualization = get_thresholded_visualization(img, pred)\n",
    "    axes[i//NB_IMAGES_PER_ROW, i%NB_IMAGES_PER_ROW].imshow(visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, this model doesn't work everywhere. We still need to train a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## People contour\n",
    "\n",
    "Now we need to draw shapes where people are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple algorithm\n",
    "\n",
    "The goal of this algorithm is simply to draw the contour of the positive predictions, without trying to draw boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_contours_from_threshold(predictions, final_shape, thickness=3):\n",
    "    height_img, width_img = final_shape\n",
    "    contour_array = np.array([[[0, 0, 0, 0] for k in range(width_img)] for l in range(height_img)])\n",
    "    height_pred, width_pred = predictions.shape\n",
    "    i_step = height_img/height_pred\n",
    "    j_step = width_img/width_pred\n",
    "    for i in range(predictions.shape[0]):\n",
    "        for j in range(predictions.shape[1]):\n",
    "            if predictions[i, j] == 1:\n",
    "                i_0 = int(i*i_step)\n",
    "                i_1 = int((i+1)*i_step)\n",
    "                j_0 = int(j*j_step)\n",
    "                j_1 = int((j+1)*j_step)\n",
    "                if i == 0 or predictions[i-1, j] == 0:\n",
    "                    contour_array[i_0:i_0+thickness, j_0:j_1] = np.array([[[255, 0, 0, 255] for k in range(j_1-j_0)] for l in range(thickness)])\n",
    "                if i == predictions.shape[0] - 1 or predictions[i+1, j] == 0:\n",
    "                    contour_array[i_1-thickness:i_1, j_0:j_1] = np.array([[[255, 0, 0, 255] for k in range(j_1-j_0)] for l in range(thickness)])\n",
    "                if j == 0 or predictions[i, j-1] == 0:\n",
    "                    contour_array[i_0:i_1, j_0:j_0+thickness] = np.array([[[255, 0, 0, 255] for k in range(thickness)] for l in range(i_1-i_0)])\n",
    "                if j == predictions.shape[1] - 1 or predictions[i, j+1] == 0:\n",
    "                    contour_array[i_0:i_1, j_1-thickness:j_1] = np.array([[[255, 0, 0, 255] for k in range(thickness)] for l in range(i_1-i_0)])\n",
    "    return contour_array.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.array([\n",
    "    [1, 1, 1, 0, 0, 0], \n",
    "    [1, 1, 1, 0, 0, 1],\n",
    "    [1, 1, 1, 0, 0, 0], \n",
    "    [0, 0, 0, 0, 1, 1],\n",
    "    [0, 0, 0, 1, 1, 1]])\n",
    "final_dim = (500, 600)\n",
    "test_contours = get_img_contours_from_threshold(test_array, final_dim)\n",
    "plt.imshow(test_contours)\n",
    "test_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_contours = get_img_contours_from_threshold(thresholded_test_predictions, IMAGE_SIZE)\n",
    "plt.imshow(img_pred)\n",
    "plt.imshow(test_contours)\n",
    "thresholded_test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_contour(predictions, image_shape):\n",
    "    reshaped_pred = predictions.reshape(predictions.shape[0], predictions.shape[1])\n",
    "    thresholded_pred = threshold_pred(reshaped_pred)\n",
    "    return get_img_contours_from_threshold(thresholded_pred, image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_IMAGES_PER_ROW = 4\n",
    "fig, axes = plt.subplots(nrows=ceil(NB_IMAGES_TO_DL/NB_IMAGES_PER_ROW), ncols=NB_IMAGES_PER_ROW, figsize=(NB_IMAGES_PER_ROW*10,ceil(NB_IMAGES_TO_DL/NB_IMAGES_PER_ROW)*10))\n",
    "plt.figure(1)\n",
    "for i, pred in enumerate(predictions_new_model):\n",
    "    img = image.load_img(get_file_path(df_final.iloc[i][\"FileName\"]), target_size=IMAGE_SIZE)\n",
    "    contours = get_img_contour(pred, IMAGE_SIZE)\n",
    "    axes[i//NB_IMAGES_PER_ROW, i%NB_IMAGES_PER_ROW].imshow(img)\n",
    "    axes[i//NB_IMAGES_PER_ROW, i%NB_IMAGES_PER_ROW].imshow(contours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now this isn't very good. We will have to try with a well trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_image(predictions):\n",
    "    img_array = np.zeros(IMAGE_SIZE)\n",
    "    height_img, width_img = IMAGE_SIZE\n",
    "    height_pred, width_pred = predictions.shape\n",
    "    i_step = height_img/height_pred\n",
    "    j_step = width_img/width_pred\n",
    "    for i in range(predictions.shape[0]):\n",
    "        for j in range(predictions.shape[1]):\n",
    "            i_0 = int(i*i_step)\n",
    "            i_1 = int((i+1)*i_step)\n",
    "            j_0 = int(j*j_step)\n",
    "            j_1 = int((j+1)*j_step)\n",
    "            img_array[i_0:i_1, j_0:j_1] = predictions[i, j]*np.ones((i_1-i_0, j_1-j_0))\n",
    "    return img_array\n",
    "\n",
    "test_pred = predictions_new_model[IMG_IND].reshape(test_pred.shape[0], test_pred.shape[1])\n",
    "pred_image = get_prediction_image(test_pred)\n",
    "print(pred_image)\n",
    "plt.imshow(pred_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_test_pred = threshold_pred(test_pred)\n",
    "th_pred_image = get_prediction_image(th_test_pred)\n",
    "print(th_pred_image)\n",
    "plt.imshow(th_pred_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.findContours(th_pred_image, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30eb72e551daf85567724a3940748a09b3df4e38abf56382eccf260bba01b7b8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
